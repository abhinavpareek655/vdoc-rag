{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b021d5",
   "metadata": {},
   "source": [
    "# Embedding & Retrieval Evaluation\n",
    "\n",
    "This notebook benchmarks embedding models and chunk sizes for retrieval quality using your project's Chroma index and collected feedback as a small labeled set. Metrics: Precision@K, Recall@K, and MRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18518993",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\abhin\\\\.ipython\\\\profile_default\\\\security'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project imports (uses your existing pipeline)\n",
    "from app.feedback_manager import _load_feedback\n",
    "from app.ingest import process_pdf\n",
    "from app.embeddings import TextImageEmbedder\n",
    "\n",
    "# Config\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "PDF_PATH = os.path.join(BASE_DIR, \"samples\", \"vdoc_rag_test.pdf\")  # replace with a real sample PDF path\n",
    "STORAGE_DIR = os.path.join(BASE_DIR, \"storage\", \"chroma_db\")\n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"multi-qa-MiniLM-L6-cos-v1\",\n",
    "    \"paraphrase-MiniLM-L3-v2\",\n",
    "    os.path.join(BASE_DIR, \"models\", \"vdoc_feedback_tuned\", \"latest\"),\n",
    "]\n",
    "CHUNK_SIZES = [200, 500, 800]  # in characters\n",
    "TOP_K = 5\n",
    "\n",
    "print(\"Notebook configured. If the tuned model path does not exist, it will be skipped in runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ba97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feedback (if available)\n",
    "feedback = _load_feedback()\n",
    "print(f\"Loaded {len(feedback)} feedback entries.\")\n",
    "if feedback:\n",
    "    sample_queries = [f['question'] for f in feedback]\n",
    "    sample_answers = [f['answer'] for f in feedback]\n",
    "else:\n",
    "    # fallback small test set\n",
    "    sample_queries = [\n",
    "        \"What is the trend in yearly sales?\",\n",
    "        \"Who scored highest in the table?\",\n",
    "        \"What is the event date?\",\n",
    "    ]\n",
    "    sample_answers = [\"increasing\", \"Charlie\", \"November 20, 2025\"]\n",
    "\n",
    "# Small helper to preview feedback structure\n",
    "if feedback:\n",
    "    display(pd.DataFrame(feedback)[['timestamp','question','answer','correctness']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0fcb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: process the PDF into chunks (optional - heavy).\n",
    "def load_chunks(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
    "    print(\"Processing PDF into chunks (this may take a while)...\")\n",
    "    docs = process_pdf(pdf_path)\n",
    "    texts = [d['text'] for d in docs]\n",
    "    return texts\n",
    "\n",
    "# Try to load sample chunks if available, otherwise create toy chunks from feedback answers\n",
    "try:\n",
    "    chunks = load_chunks(PDF_PATH)\n",
    "    print(f\"Total chunks from PDF: {len(chunks)}\")\n",
    "except Exception as e:\n",
    "    print(\"Could not process PDF, falling back to feedback-derived tiny corpus:\", e)\n",
    "    # fallback corpus built from sample answers/queries for quick runs\n",
    "    chunks = [\n",
    "        \"Yearly sales have been increasing steadily from 2018 to 2024, with a notable jump in 2021.\",\n",
    "        \"Charlie achieved the highest score in the table with 98 points.\",\n",
    "        \"The event will be held on November 20, 2025 at the downtown auditorium.\",\n",
    "    ]\n",
    "    print(f\"Using fallback chunks: {len(chunks)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function (Precision@K, Recall@K, MRR)\n",
    "def evaluate_model(model_name, chunks, queries, answers, chunk_size, top_k=TOP_K):\n",
    "    print(f\"\\nðŸ§  Evaluating {model_name} (chunk size {chunk_size})\")\n",
    "    # Skip model if path does not exist (for tuned model)\n",
    "    if os.path.isabs(model_name) and not os.path.exists(model_name):\n",
    "        print(f\"- Skipping (path not found): {model_name}\")\n",
    "        return None\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Split chunks by size\n",
    "    split_chunks = []\n",
    "    for ch in chunks:\n",
    "        for i in range(0, len(ch), chunk_size):\n",
    "            split_chunks.append(ch[i:i+chunk_size])\n",
    "    chunk_embeddings = model.encode(split_chunks, normalize_embeddings=True, show_progress_bar=False)\n",
    "\n",
    "    precision_scores, recall_scores, mrr_scores = [], [], []\n",
    "\n",
    "    # Precompute reference counts for recall denominator\n",
    "    total_relevant_counts = []\n",
    "    for ans in answers:\n",
    "        total_relevant_counts.append(sum(1 for c in split_chunks if ans.lower() in c.lower()))\n",
    "\n",
    "    for q, ans in tqdm(list(zip(queries, answers)), total=len(queries), desc=f\"Evaluating {model_name}\"):\n",
    "        qvec = model.encode([q], normalize_embeddings=True)\n",
    "        sims = cosine_similarity(qvec, chunk_embeddings)[0]\n",
    "        top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "        retrieved_chunks = [split_chunks[i] for i in top_indices]\n",
    "\n",
    "        relevant = [1 if ans.lower() in c.lower() else 0 for c in retrieved_chunks]\n",
    "        precision = sum(relevant) / top_k\n",
    "        recall = sum(relevant) / max(1, total_relevant_counts.pop(0))\n",
    "        mrr = 0.0\n",
    "        for rank, rel in enumerate(relevant, start=1):\n",
    "            if rel == 1:\n",
    "                mrr = 1.0 / rank\n",
    "                break\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"precision\": float(np.mean(precision_scores)),\n",
    "        \"recall\": float(np.mean(recall_scores)),\n",
    "        \"mrr\": float(np.mean(mrr_scores)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca934bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation across models and chunk sizes\n",
    "results = []\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    for cs in CHUNK_SIZES:\n",
    "        res = evaluate_model(model_name, chunks, sample_queries, sample_answers, cs)\n",
    "        if res:\n",
    "            results.append(res)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "if not df.empty:\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results to show (models may have been skipped).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f75729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if not df.empty:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for m in df['model'].unique():\n",
    "        subset = df[df['model'] == m]\n",
    "        plt.plot(subset['chunk_size'], subset['precision'], marker='o', label=f\"{m} (Precision)\")\n",
    "    plt.title('Precision@5 vs Chunk Size')\n",
    "    plt.xlabel('Chunk Size (characters)')\n",
    "    plt.ylabel('Precision@5')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for m in df['model'].unique():\n",
    "        subset = df[df['model'] == m]\n",
    "        plt.plot(subset['chunk_size'], subset['recall'], marker='s', label=f\"{m} (Recall)\")\n",
    "    plt.title('Recall@5 vs Chunk Size')\n",
    "    plt.xlabel('Chunk Size (characters)')\n",
    "    plt.ylabel('Recall@5')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for reporting\n",
    "output_csv = os.path.join(BASE_DIR, 'notebooks', 'embedding_benchmark_results.csv')\n",
    "if not df.empty:\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"âœ… Benchmark results saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
